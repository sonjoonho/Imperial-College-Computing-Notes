We are using quadratic loss, so we have for the loss function $L$
$$
\begin{align*}
L &= \frac{1}{2} \frac{1}{N} \sum_i^N (\hat{y}_1^{(i)} - y_1^{(i)})^2 + (\hat{y}_2^{(i)} - y_2^{(i)})^2
\end{align*}
$$
using superscript to denote the sample index, and subscript to denote the feature index. Note that we are normalising by (1) the number of output neurons and (2) the batch size. To derive the update rule
$$
W \leftarrow W - \alpha \frac{\partial L}{\partial W}
$$
we need to find $\frac{\partial L}{\partial W}$. By the chain rule we have that
$$
\begin{align*}
\frac{\partial L}{\partial W} &= \frac{\partial L}{\partial Z} \frac{\partial Z}{\partial W}
\end{align*}
$$
and taking the derivative of $L$ (a scalar) by $Z$ (a matrix) we find
$$
\begin{align*}
\frac{\partial L}{\partial Z} &= 
\begin{bmatrix}
\frac{\partial L}{\partial Z_{1, 1}} & \frac{\partial L}{\partial Z_{1, 2}} \\
\frac{\partial L}{\partial Z_{2, 1}} & \frac{\partial L}{\partial Z_{2, 2}} 
\end{bmatrix}
\end{align*}
$$

We can see that the loss function can be expressed as
$$
L = \frac{1}{2} \frac{1}{N} \left( (\sigma(Z_{1, 1}) - Y_{1, 1})^2 + (\sigma(Z_{1, 2}) - Y_{1, 2})^2 + (\sigma(Z_{2, 1}) - Y_{2, 1})^2 + (\sigma(Z_{2, 2}) - Y_{2, 2})^2 \right)
$$
and so looking at one element
$$
\frac{\partial L}{\partial Z_{1, 1}} = \frac{1}{N} \sigma'(Z_{1,1}) (\hat{y}^{(1)}_1 - y^{(1)}_1)
$$
so for the whole matrix
$$
\frac{\partial L}{\partial Z} = \frac{1}{N} \sigma'(Z) \circ (\hat{Y} - Y)
$$
and we also know that 
$$
\frac{\partial Z}{\partial W} = X^T
$$
and since we know that $X^T \in \mathbb{R}^{3\times2}$ and $\frac{\partial L}{\partial Z} \in \mathbb{R}^{2\times2}$, we have
$$
\frac{\partial L}{\partial W} = X^T \frac{1}{N}\sigma'(Z) \circ (\hat{Y} - Y)
$$

